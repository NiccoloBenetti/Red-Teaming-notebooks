{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Red teaming Agent in AI Foundry (Project based resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../infra/credentials.env\", override=True)\n",
    "\n",
    "# print some varaibles to check the process\n",
    "print(\"Project endpoint:\", os.getenv(\"AZURE_PJ_PROJECT_ENDPOINT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create an AI Red Teaming Agent\n",
    "You can instantiate the AI Red Teaming agent with your Azure AI Project and Azure Credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure imports\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation.red_team import RedTeam, RiskCategory\n",
    "\n",
    "# Azure AI Project Information (unique for the project approach)\n",
    "azure_ai_project = os.environ.get(\"AZURE_PJ_PROJECT_ENDPOINT\")\n",
    "\n",
    "# Instantiate your AI Red Teaming Agent\n",
    "red_team_agent = RedTeam(\n",
    "    azure_ai_project=azure_ai_project,     # required\n",
    "    credential=DefaultAzureCredential(),   # required\n",
    "    risk_categories=[                      # optional, defaults to all four risk categories\n",
    "        RiskCategory.Violence\n",
    "        # RiskCategory.HateUnfairness,\n",
    "        # RiskCategory.Sexual,\n",
    "        # RiskCategory.SelfHarm\n",
    "    ], \n",
    "    num_objectives=1,                      # optional, defaults to 10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Optionally, you can specify which **risk categories** of content risks you want to cover with `risk_categories` and define the **number of prompts** covering each risk category with `num_objectives`. The previous example generates 5 seed prompts for each risk category for a total of 20 rows of prompts to be generated and sent to your target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Targets You Can Scan with the AI Red Teaming Agent\n",
    "\n",
    "When using the AI Red Teaming Agent, you must define a **target**, which is the AI system or interface you want to probe for safety risks. The agent supports several types of targets, depending on how your AI system is deployed or accessed:\n",
    "\n",
    "### 1 - Model Configuration\n",
    "Use this when scanning a deployed model, such as an Azure OpenAI deployment. This is ideal during model selection or benchmarking phases.\n",
    "\n",
    "### 2 - Simple Callback\n",
    "Use a basic Python function that receives a prompt and returns a response. Perfect if you have a custom application (e.g., a chatbot or RAG pipeline) and want to test it easily.\n",
    "\n",
    "### 3 - Advanced Callback (Chat Protocol)\n",
    "Use this when your AI system handles structured chat messages (with roles like `user`, `assistant`). This is great for systems that follow the OpenAI chat format or multi-turn logic.\n",
    "\n",
    "### 4 - PyRIT Prompt Target\n",
    "For advanced users already working with PyRIT. This option allows you to integrate existing PyRIT targets like `OpenAIChatTarget` or any class inheriting from `PromptChatTarget`.\n",
    "\n",
    "Each of these target types enables the agent to send adversarial prompts and evaluate how your system responds to potentially harmful or sensitive queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Scanning an Azure OpenAI Model (Model Configuration)\n",
    "\n",
    "In this example, we'll scan an Azure OpenAI model by passing its configuration details as the target to the AI Red Teaming Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Azure OpenAI model\n",
    "azure_openai_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_PJ_MODEL_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_PJ_MODEL_NAME\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_PJ_MODEL_API_KEY\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_team_result = await red_team_agent.scan(target=azure_openai_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Strategies Overview\n",
    "\n",
    "By default, if you run a scan without specifying any attack strategies, the AI Red Teaming Agent will use **baseline direct adversarial prompts**. These are straightforward queries aimed at triggering undesired behavior, and are a great starting point to evaluate initial model alignment.\n",
    "\n",
    "### What Are Attack Strategies?\n",
    "Attack strategies are techniques that **modify baseline adversarial prompts** to try and **bypass your AI systemâ€™s safeguards**. They simulate how a real adversary might attempt to \"trick\" the system.\n",
    "\n",
    "### Levels of Attack Complexity\n",
    "\n",
    "Attack strategies are grouped into three complexity levels based on the effort needed to perform them:\n",
    "\n",
    "- **Easy**  \n",
    "  Simple transformations, such as encoding or character swaps.  \n",
    "  _Strategies: `Base64`, `Flip`, `Morse`_\n",
    "\n",
    "- **Moderate**  \n",
    "  Require extra resources like a generative model to reformulate prompts.  \n",
    "  _Strategies: `Tense`_\n",
    "\n",
    "- **Difficult**  \n",
    "  Combine multiple strategies or rely on advanced techniques like search-based prompt optimization.  \n",
    "  _Strategies: A composition of `Tense` + `Base64`_\n",
    "\n",
    "You can specify individual strategies or use predefined complexity groups in the `attack_strategies` parameter during a scan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example using default grouped attack strategies\n",
    "The following scan would first run all the baseline direct adversarial queries. Then, it would apply the following attack techniques: Base64, Flip, Morse, Tense, and a composition of Tense and Base64 which would first translate the baseline query into past tense then encode it into Base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.red_team import AttackStrategy\n",
    "\n",
    "# Run the red team scan with default grouped attack strategies\n",
    "red_team_agent_result = await red_team_agent.scan(\n",
    "    target=azure_openai_config, # required\n",
    "    output_path=\"Scan results/My-First-RedTeam-Scan.json\", #optional, output file\n",
    "    scan_name=\"Scan with many strategies\", # optional, names your scan in Azure AI Foundry\n",
    "    attack_strategies=[ # optional\n",
    "        AttackStrategy.EASY, \n",
    "        AttackStrategy.MODERATE,  \n",
    "        AttackStrategy.DIFFICULT,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example using specific attack strategies\n",
    "More advanced users can specify the desired attack strategies instead of using default groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the red team scan with multiple attack strategies\n",
    "red_team_agent_result = await red_team_agent.scan(\n",
    "    target=azure_openai_config, # required\n",
    "    scan_name=\"Scan with many strategies\", # optional\n",
    "    attack_strategies=[ # optional\n",
    "        AttackStrategy.CharacterSpace,  # Add character spaces\n",
    "        AttackStrategy.ROT13,  # Use ROT13 encoding\n",
    "        AttackStrategy.UnicodeConfusable,  # Use confusable Unicode characters\n",
    "        AttackStrategy.Compose([AttackStrategy.Base64, AttackStrategy.ROT13]), # composition of strategies\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Your Scan Results\n",
    "\n",
    "Once your automated red teaming scan is complete, the results are summarized in a detailed scorecard. The key metric to look at is the **Attack Success Rate (ASR)**, which measures the percentage of attack prompts that successfully triggered undesirable or unsafe responses from your AI system.\n",
    "\n",
    "You can specify an `output_path` when running your scan to save the results in a JSON file. This file includes:\n",
    "\n",
    "- **ASR breakdown by risk category** (e.g., violence, self-harm, etc.)\n",
    "- **ASR breakdown by attack complexity** (baseline, easy, moderate, difficult)\n",
    "- **Joint summaries** that correlate attack strategies with specific risk categories\n",
    "- **Simulation parameters** showing which categories and attack techniques were used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Red Teaming scan results JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the output JSON file from the scan\n",
    "output_file_path = \"Scan results/My-First-RedTeam-Scan.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(output_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    red_team_results = json.load(file)\n",
    "\n",
    "# Pretty print the JSON with indentation and Unicode support\n",
    "formatted_json = json.dumps(red_team_results, indent=2, ensure_ascii=False)\n",
    "print(formatted_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring your own objectives: Using your own prompts as objectives for RedTeam\n",
    "   Below we demonstrate how to use your own prompts as objectives for a `RedTeam` scan. You can see the required format for prompts under `.\\assets\\adversarial-prompts.json`. Note that when bringing your own prompts, the supported `risk-types` are `violence`, `sexual`, `hate_unfairness`, and `self_harm`. The number of prompts you specify will be the `num_objectives` used in the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_prompts = \"..\\\\assets\\\\substance-abuse-prompts.json\"\n",
    "\n",
    "# Create the RedTeam specifying the custom attack seed prompts to use as objectives\n",
    "custom_red_team = RedTeam(\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    credential=DefaultAzureCredential(),\n",
    "    custom_attack_seed_prompts=path_to_prompts,  # Path to a file containing custom attack seed prompts\n",
    ")\n",
    "\n",
    "custom_red_team_result = await custom_red_team.scan(\n",
    "    target=azure_openai_config,\n",
    "    scan_name=\"Custom-Prompt-Scan\",\n",
    "    attack_strategies=[\n",
    "        AttackStrategy.EASY,\n",
    "        AttackStrategy.MODERATE,  \n",
    "        AttackStrategy.DIFFICULT, \n",
    "    ],\n",
    "    output_path=\"Scan results/Custom-Prompt-Scan.json\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
